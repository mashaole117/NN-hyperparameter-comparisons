{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation cell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this prokect I have made use of the [video game sales](https://www.kaggle.com/gregorut/videogamesales) dataset from kaggle which contains the sales rankings of videos games that have sold over 100 000 copies in several market regions. The columns from the dataset that will be extracted as input features are the platform the game was released on, the genre the game is classified under adn the pulisher of the videogame.\n",
    "\n",
    "I will be using a neural network with a variety of hyperparameters, including various activation functions, namely: linear, sigmoid, tanh and ReLU, to see how they affect the accuracy of prediction. Comparisions will be made between three different numbers of hidden layes, namely: one, three and five, to see how these variations affect the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "I will be using a pandas dataframe to import the data that is in CSV format. Only the columns related to the name of the game, its release platform, the genre, and the publisher will be extracted as input features. The column containing the global sales (Global_Sales, in millions) will also be extracted as the response variable. \n",
    "\n",
    "Once the data is loaded into a pandas data frame, any observations containg a column with a null value are culled from the data frame. This may introduce bias by removing samples from the dataset, however only a few observations needed to be removed so the effect should be minor.\n",
    "\n",
    "The Platform, Genre and Publisher columns contain object/string values, meaning these need to be encoded into integers to allow for their use in the neural network. Simple label encoding is used to give each unique string value in a column an associated integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('vgsales.csv',index_col=0)\n",
    "\n",
    "df = df.loc[:,['Name','Platform','Genre','Publisher','Global_Sales']]\n",
    "df = df.dropna(axis='index') # removing all rows containg null values (might cause biasing)\n",
    "df = df.drop_duplicates() # remove any duplicate entries in the dataset\n",
    "\n",
    "# Converting object types to category types\n",
    "df.Platform = df.Platform.astype('category')\n",
    "df.Genre = df.Genre.astype('category')\n",
    "df.Publisher = df.Publisher.astype('category')\n",
    "\n",
    "# Label encoding for all columns containing string values\n",
    "df['Platform_cat'] = df.Platform.cat.codes\n",
    "df['Publisher_cat'] = df.Publisher.cat.codes\n",
    "df['Genre_cat'] = df.Genre.cat.codes\n",
    "df = df.reset_index()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation\n",
    "Below is a pair plot which can be used as early indicator of any correlation between attributes of the data. It is clear that looking at the distribution of the Global_Sales column, there are outliers skewing the data at the right tail, meaning these need to be removed in order to obtain more representative data that will allow the neural net to be more generalizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting distribution of sales\n",
    "plt.plot(df.Global_Sales)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers in data\n",
    "df = df[df.Global_Sales.between(df.Global_Sales.quantile(.15), df.Global_Sales.quantile(.85))]\n",
    "plt.plot(df.Global_Sales)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA to find correlations in data\n",
    "print(df.corr()) # correlation matrix\n",
    "# Correlation Visualization\n",
    "cols = ['Platform_cat', 'Genre_cat', 'Publisher_cat', 'Global_Sales']\n",
    "pp = sns.pairplot(df[cols], height=1.5, aspect=1.5,plot_kws=dict(edgecolor=\"k\", linewidth=0.5),diag_kind=\"kde\", diag_kws=dict(shade=True))\n",
    "fig = pp.fig \n",
    "fig.subplots_adjust(top=0.93, wspace=0.3)\n",
    "t = fig.suptitle('Video game sales attributes', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting test and training data\n",
    "X = df.loc[:,['Platform_cat', 'Genre_cat', 'Publisher_cat']]\n",
    "Y = df.loc[:,\"Global_Sales\"]\n",
    "X_train,X_test,Y_train,Y_test = tts(X.values, Y.values, test_size = 0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementations\n",
    "The neural network will be implement with varying hyperparameters. Initially a single hidden layer will be used along with and input layer that has three input units and one output unit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "\n",
    "# Sigmoid function\n",
    "def sig(z): \n",
    "    return 1/(1+np.exp(-z))\n",
    "# Linear function\n",
    "def lin(z):\n",
    "    return z\n",
    "# tanh function\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "# ReLU\n",
    "def relu(z):\n",
    "    return np.maximum(0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random initialization for weights\n",
    "def rand_weights(dim_layers):\n",
    "    weights = {}\n",
    "    np.random.seed(25) # ensure reproducibility\n",
    "    \n",
    "    for i in range(1,len(dim_layers)):\n",
    "        weights[i] = np.random.randn(dim_layers[i], dim_layers[i-1])/100\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foward propagation using different activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation using sigmoid\n",
    "\n",
    "def for_prop_sig(x, weights):\n",
    "    l = len(weights)\n",
    "    a = {}\n",
    "    z = {}\n",
    "\n",
    "    z[1] = np.dot(weights[1], x) + 1\n",
    "    a[1] = sig(z[1])\n",
    "\n",
    "    for i in range(2, l+1):\n",
    "        z[i] = np.dot(weights[i], a[i-1]) + 1\n",
    "        if i==l:\n",
    "            a[i] = z[i]\n",
    "        else:\n",
    "            a[i] = sig(z[i])\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation using linear\n",
    "\n",
    "def for_prop_lin(x, weights):\n",
    "    l = len(weights)\n",
    "    a = {}\n",
    "    z = {}\n",
    "    \n",
    "    z[1] = np.dot(weights[1], x) + 1\n",
    "    a[1] = sig(z[1])\n",
    "\n",
    "    for i in range(2, l+1):\n",
    "        z[i] = np.dot(weights[i], a[i-1]) + 1\n",
    "        if i==l:\n",
    "            a[i] = z[i]\n",
    "        else:\n",
    "            a[i] = lin(z[i])\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation using tanh\n",
    "\n",
    "def for_prop_tanh(x, weights):\n",
    "    l = len(weights)\n",
    "    a = {}\n",
    "    z = {}\n",
    "    \n",
    "    z[1] = np.dot(weights[1], x) + 1\n",
    "    a[1] = sig(z[1])\n",
    "\n",
    "    for i in range(2, l+1):\n",
    "        z[i] = np.dot(weights[i], a[i-1]) + 1\n",
    "        if i==l:\n",
    "            a[i] = z[i]\n",
    "        else:\n",
    "            a[i] = tanh(z[i])\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation using relu\n",
    "\n",
    "def for_prop_relu(x, weights):\n",
    "    l = len(weights)\n",
    "    a = {}\n",
    "    z = {}\n",
    "    \n",
    "    z[1] = np.dot(weights[1], x) + 1\n",
    "    a[1] = sig(z[1])\n",
    "\n",
    "    for i in range(2, l+1):\n",
    "        z[i] = np.dot(weights[i], a[i-1]) + 1\n",
    "        if i==l:\n",
    "            a[i] = z[i]\n",
    "        else:\n",
    "            a[i] = relu(z[i])\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost calculation\n",
    "\n",
    "def cost_calc(a,y):\n",
    "    l = len(a)\n",
    "\n",
    "    hyp = a[l]\n",
    "    cost = 1/(2*len(y)) * np.sum(np.square(hyp - y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation\n",
    "\n",
    "def back_prop(weights,a,x,y):\n",
    "    l = len(weights)\n",
    "    n = len(y)\n",
    "    deltas = {}\n",
    "  \n",
    "    for i in range(l,0,-1):\n",
    "        if i==l:\n",
    "            da = 1/n*(a[i] - y)\n",
    "            dz = da\n",
    "        else:\n",
    "            da = np.dot(weights[i+1].T, dz)\n",
    "            dz = np.multiply(da, a[i])\n",
    "        if i==1:\n",
    "            deltas[i] = (1/n)*np.dot(dz, x.T)\n",
    "        else:\n",
    "            deltas[i] = (1/n)*np.dot(dz,a[i-1].T)\n",
    "    \n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weights based on errors\n",
    "\n",
    "def new_weights(weights,deltas,alpha):\n",
    "    l = len(weights)\n",
    "\n",
    "    n_weights = {}\n",
    "    \n",
    "    for i in range(1,l+1):\n",
    "        n_weights[i] = weights[i] - alpha*deltas[i]\n",
    "    \n",
    "    return n_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using different activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of model using sigmoid activation functions\n",
    "\n",
    "def train_sig(x, y, dim_layers, iterations, alpha):\n",
    "    weights = rand_weights(dim_layers)\n",
    "    iter_costs = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        a = for_prop_sig(x.T, weights)\n",
    "        cost = cost_calc(a, y.T)\n",
    "        deltas = back_prop(weights,a,x.T,y.T)\n",
    "        weights = new_weights(weights,deltas,alpha)\n",
    "        iter_costs.append(cost)\n",
    "    \n",
    "    return weights,iter_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of model using linear activation functions\n",
    "\n",
    "def train_lin(x, y, dim_layers, iterations, alpha):\n",
    "    weights = rand_weights(dim_layers)\n",
    "    iter_costs = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        a = for_prop_lin(x.T, weights)\n",
    "        cost = cost_calc(a, y.T)\n",
    "        deltas = back_prop(weights,a,x.T,y.T)\n",
    "        weights = new_weights(weights,deltas,alpha)\n",
    "        iter_costs.append(cost)\n",
    "    \n",
    "    return weights,iter_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of model using tanh activation functions\n",
    "\n",
    "def train_tanh(x, y, dim_layers, iterations, alpha):\n",
    "    weights = rand_weights(dim_layers)\n",
    "    iter_costs = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        a = for_prop_tanh(x.T, weights)\n",
    "        cost = cost_calc(a, y.T)\n",
    "        deltas = back_prop(weights,a,x.T,y.T)\n",
    "        weights = new_weights(weights,deltas,alpha)\n",
    "        iter_costs.append(cost)\n",
    "    \n",
    "    return weights,iter_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of model using ReLU activation functions\n",
    "\n",
    "def train_relu(x, y, dim_layers, iterations, alpha):\n",
    "    weights = rand_weights(dim_layers)\n",
    "    iter_costs = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        a = for_prop_relu(x.T, weights)\n",
    "        cost = cost_calc(a, y.T)\n",
    "        deltas = back_prop(weights,a,x.T,y.T)\n",
    "        weights = new_weights(weights,deltas,alpha)\n",
    "        iter_costs.append(cost)\n",
    "    \n",
    "    return weights,iter_costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean square error calculations for various activation function predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy of sigmoid activation function\n",
    "\n",
    "def sig_err(x_train, x_test, y_train, y_test, weights,dim_layers):\n",
    "    \n",
    "    a_test = for_prop_sig(x_test.T, weights)\n",
    "    a_train = for_prop_sig(x_train.T, weights)\n",
    "\n",
    "    test_err = mse(y_test, a_test[len(dim_layers)-1].T)\n",
    "    train_err = mse(y_train, a_train[len(dim_layers)-1].T)\n",
    "    \n",
    "    return test_err, train_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy of linear activation function\n",
    "\n",
    "def lin_err(x_train, x_test, y_train, y_test, weights,dim_layers):\n",
    "\n",
    "    a_test = for_prop_lin(x_test.T, weights)\n",
    "    a_train = for_prop_lin(x_train.T, weights)\n",
    "\n",
    "    test_err = mse(y_test, a_test[len(dim_layers)-1].T)\n",
    "    train_err = mse(y_train, a_train[len(dim_layers)-1].T)\n",
    "    \n",
    "    return test_err, train_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy of tanh activation function\n",
    "\n",
    "def tanh_err(x_train, x_test, y_train, y_test, weights,dim_layers):\n",
    "\n",
    "    a_test = for_prop_tanh(x_test.T, weights)\n",
    "    a_train = for_prop_tanh(x_train.T, weights)\n",
    "\n",
    "    test_err = mse(y_test, a_test[len(dim_layers)-1].T)\n",
    "    train_err = mse(y_train, a_train[len(dim_layers)-1].T)\n",
    "    \n",
    "    return test_err, train_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy of relu activation function\n",
    "\n",
    "def relu_err(x_train, x_test, y_train, y_test, weights,dim_layers):\n",
    "\n",
    "    a_test = for_prop_relu(x_test.T, weights)\n",
    "    a_train = for_prop_relu(x_train.T, weights)\n",
    "\n",
    "    test_err = mse(y_test, a_test[len(dim_layers)-1].T)\n",
    "    train_err = mse(y_train, a_train[len(dim_layers)-1].T)\n",
    "    \n",
    "    return test_err, train_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paramenters\n",
    "iterations = 1000\n",
    "alpha = 0.05\n",
    "\n",
    "# Varoius neural net hidden layer dimensions\n",
    "dim_layers1 = [3,20,1]\n",
    "dim_layers2 = [3,10,10,1]\n",
    "dim_layers3 = [3,5,5,5,1]\n",
    "dim_layers4 = [3,10,10,10,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test Using Sigmoid Activation Function\n",
    "The following contains the training for a neural network using the sigmoid activation function and various dimensions for the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating weights and costs over iterations for various hidden layer dimensions\n",
    "\n",
    "weights_sig1,costs_sig1 = train_sig(X_train, Y_train, dim_layers1, iterations, alpha)\n",
    "weights_sig2,costs_sig2 = train_sig(X_train, Y_train, dim_layers2, iterations, alpha)\n",
    "weights_sig3,costs_sig3 = train_sig(X_train, Y_train, dim_layers3, iterations, alpha)\n",
    "weights_sig4,costs_sig4 = train_sig(X_train, Y_train, dim_layers4, iterations, alpha)\n",
    "\n",
    "# Error calculations for various hidden layer dimensions\n",
    "\n",
    "test_err_sig1, train_err_sig1 = sig_err(X_train, X_test, Y_train, Y_test, weights_sig1,dim_layers1)\n",
    "test_err_sig2, train_err_sig2 = sig_err(X_train, X_test, Y_train, Y_test, weights_sig2,dim_layers2)\n",
    "test_err_sig3, train_err_sig3 = sig_err(X_train, X_test, Y_train, Y_test, weights_sig3,dim_layers3)\n",
    "test_err_sig4, train_err_sig4 = sig_err(X_train, X_test, Y_train, Y_test, weights_sig4,dim_layers4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of iteration costs for various hidden layer dimensions\n",
    "\n",
    "iter = np.arange(1,1001)\n",
    "figure = plt.figure(figsize=(7,6))\n",
    "ax = figure.add_subplot(1,1,1)\n",
    "ax.plot(iter,costs_sig1,'r',label='1 hidden layer of 20 unit')\n",
    "ax.plot(iter,costs_sig2,'g',label='2 hidden layers of 10 units')\n",
    "ax.plot(iter,costs_sig3,'b',label='3 hidden layers of 5 units')\n",
    "ax.plot(iter,costs_sig4,'orange',label='3 hidden layers of 10 units')\n",
    "ax.set_xlabel('Iterations');ax.set_ylabel('Costs');ax.set_title('Cost reduction using sigmoid function');ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, it can be seen that a network architecture that contains 1 hidden layer of 20 units has costs that converges fastest over the 1000 iterations. The configurations using 2 hidden layers starts off with a lower cost than using 3 hidden layers, but converges at a similar rate to using 3 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of errors for various hidden layer dimensions\n",
    "\n",
    "figure = plt.figure(figsize=(7,6))\n",
    "ax = figure.add_subplot(1,1,1)\n",
    "ax.scatter(x=test_err_sig1,y=train_err_sig1,c='r',label='1 hidden layer of 20 unit')\n",
    "ax.scatter(x=test_err_sig2,y=train_err_sig2,c='g',label='2 hidden layers of 10 units')\n",
    "ax.scatter(x=test_err_sig3,y=train_err_sig3,c='b',label='3 hidden layers of 5 units')\n",
    "ax.scatter(x=test_err_sig4,y=train_err_sig4,c='orange',label='3 hidden layers of 10 units')\n",
    "ax.set_xlabel('Train error');ax.set_ylabel('Test error');ax.set_title('Errors using sigmoid function');ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, it can be seen that using 1 hidden layer consisting of 20 units each gives the least train and testing error, followed by the use of 2 hidden layer with 10 units. This is consistent with the convergence rates from the previous plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test Using Linear Activation Function\n",
    "The following contains the training for a neural network using the linear activation function and various dimensions for the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating weights and costs over iterations for various hidden layer dimensions\n",
    "\n",
    "weights_lin1,costs_lin1 = train_lin(X_train, Y_train, dim_layers1, iterations, alpha)\n",
    "weights_lin2,costs_lin2 = train_lin(X_train, Y_train, dim_layers2, iterations, alpha)\n",
    "weights_lin3,costs_lin3 = train_lin(X_train, Y_train, dim_layers3, iterations, alpha)\n",
    "weights_lin4,costs_lin4 = train_lin(X_train, Y_train, dim_layers4, iterations, alpha)\n",
    "\n",
    "# Error calculations for various hidden layer dimensions\n",
    "\n",
    "test_err_lin1, train_err_lin1 = lin_err(X_train, X_test, Y_train, Y_test, weights_lin1,dim_layers1)\n",
    "test_err_lin2, train_err_lin2 = lin_err(X_train, X_test, Y_train, Y_test, weights_lin2,dim_layers2)\n",
    "test_err_lin3, train_err_lin3 = lin_err(X_train, X_test, Y_train, Y_test, weights_lin3,dim_layers3)\n",
    "test_err_lin4, train_err_lin4 = lin_err(X_train, X_test, Y_train, Y_test, weights_lin4,dim_layers4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of iteration costs for various hidden layer dimensions\n",
    "\n",
    "iter = np.arange(1,1001)\n",
    "figure = plt.figure(figsize=(7,6))\n",
    "ax = figure.add_subplot(1,1,1)\n",
    "ax.plot(iter,costs_lin1,'r',label='1 hidden layer of 20 unit')\n",
    "ax.plot(iter,costs_lin2,'g',label='2 hidden layers of 10 units')\n",
    "ax.plot(iter,costs_lin3,'b',label='3 hidden layers of 5 units')\n",
    "ax.plot(iter,costs_lin4,'orange',label='3 hidden layers of 10 units')\n",
    "ax.set_xlabel('Iterations');ax.set_ylabel('Costs');ax.set_title('Cost reduction using linear function');ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the abover graph, the use of a single hidden layer with 20 units produced the fastest convergence of the cost, followed by the use of 2 hidden layers with 10 units in each layer, which starts off at a lower cost value but converges slower than using 1 hidden layer. Using a 3 hidden layer configuration consisting of 5 units in each layer produced the worst results. This shows that the linear function favours the use of a large number of units in a layer over the use of large number of layers with few units in each layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of errors for various hidden layer dimensions\n",
    "\n",
    "figure = plt.figure(figsize=(7,6))\n",
    "ax = figure.add_subplot(1,1,1)\n",
    "ax.scatter(x=test_err_lin1,y=train_err_lin1,c='r',label='1 hidden layer of 20 unit')\n",
    "ax.scatter(x=test_err_lin2,y=train_err_lin2,c='g',label='2 hidden layers of 10 units')\n",
    "ax.scatter(x=test_err_lin3,y=train_err_lin3,c='b',label='3 hidden layers of 5 units')\n",
    "ax.scatter(x=test_err_lin4,y=train_err_lin4,c='orange',label='3 hidden layers of 10 units')\n",
    "ax.set_xlabel('Train error');ax.set_ylabel('Test error');ax.set_title('Errors using linear function');ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy when using a linear function is optimal in this project when using 2 hidden layers with 10 units in each layer, followed by the use of a 1 hidden layer with 10 units in each layer. In this case, the most accurate and fastest converging configuration is the same, namely: 1 hidden layer with 20 units each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test Using tanh Activation Function\n",
    "The following contains the training for a neural network using the linear activation function and various dimensions for the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating weights and costs over iterations for various hidden layer dimensions\n",
    "\n",
    "weights_tanh1,costs_tanh1 = train_tanh(X_train, Y_train, dim_layers1, iterations, alpha)\n",
    "weights_tanh2,costs_tanh2 = train_tanh(X_train, Y_train, dim_layers2, iterations, alpha)\n",
    "weights_tanh3,costs_tanh3 = train_tanh(X_train, Y_train, dim_layers3, iterations, alpha)\n",
    "weights_tanh4,costs_tanh4 = train_tanh(X_train, Y_train, dim_layers4, iterations, alpha)\n",
    "\n",
    "# Error calculations for various hidden layer dimensions\n",
    "\n",
    "test_err_tanh1, train_err_tanh1 = tanh_err(X_train, X_test, Y_train, Y_test, weights_tanh1,dim_layers1)\n",
    "test_err_tanh2, train_err_tanh2 = tanh_err(X_train, X_test, Y_train, Y_test, weights_tanh2,dim_layers2)\n",
    "test_err_tanh3, train_err_tanh3 = tanh_err(X_train, X_test, Y_train, Y_test, weights_tanh3,dim_layers3)\n",
    "test_err_tanh4, train_err_tanh4 = tanh_err(X_train, X_test, Y_train, Y_test, weights_tanh4,dim_layers4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of iteration costs for various hidden layer dimensions\n",
    "\n",
    "iter = np.arange(1,1001)\n",
    "figure = plt.figure(figsize=(7,6))\n",
    "ax = figure.add_subplot(1,1,1)\n",
    "ax.plot(iter,costs_tanh1,'r',label='1 hidden layer of 20 unit')\n",
    "ax.plot(iter,costs_tanh2,'g',label='2 hidden layers of 10 units')\n",
    "ax.plot(iter,costs_tanh3,'b',label='3 hidden layers of 5 units')\n",
    "ax.plot(iter,costs_tanh4,'orange',label='3 hidden layers of 10 units')\n",
    "ax.set_xlabel('Iterations');ax.set_ylabel('Costs');ax.set_title('Cost reduction using tanh function');ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a tanh activation function, the 1 hidden layer configuration with 20 units converges fastest, followed by the configuration using 3 hidden layers with 10 units. The configurations using 2 and 3 hidden layers start off with lower cost values than using a single hidden layer, but converge slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of errors for various hidden layer dimensions\n",
    "\n",
    "figure = plt.figure(figsize=(7,6))\n",
    "ax = figure.add_subplot(1,1,1)\n",
    "ax.scatter(x=test_err_tanh1,y=train_err_tanh1,c='r',label='1 hidden layer of 20 unit')\n",
    "ax.scatter(x=test_err_tanh2,y=train_err_tanh2,c='g',label='2 hidden layers of 10 units')\n",
    "ax.scatter(x=test_err_tanh3,y=train_err_tanh3,c='b',label='3 hidden layers of 5 units')\n",
    "ax.scatter(x=test_err_tanh4,y=train_err_tanh4,c='orange',label='3 hidden layers of 10 units')\n",
    "ax.set_xlabel('Train error');ax.set_ylabel('Test error');ax.set_title('Errors using tanh function');ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, using 1 hidden layer leads to the least error, followed by using 2 hidden layers with 10 units each. This corresponds with convergence rates plotted before and shows than tanh generealizes better with more units used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test Using ReLU Activation Function\n",
    "The following contains the training for a neural network using the ReLU activation function and various dimensions for the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating weights and costs over iterations for various hidden layer dimensions\n",
    "\n",
    "weights_relu1,costs_relu1 = train_relu(X_train, Y_train, dim_layers1, iterations, alpha)\n",
    "weights_relu2,costs_relu2 = train_relu(X_train, Y_train, dim_layers2, iterations, alpha)\n",
    "weights_relu3,costs_relu3 = train_relu(X_train, Y_train, dim_layers3, iterations, alpha)\n",
    "weights_relu4,costs_relu4 = train_relu(X_train, Y_train, dim_layers4, iterations, alpha)\n",
    "\n",
    "# Error calculations for various hidden layer dimensions\n",
    "\n",
    "test_err_relu1, train_err_relu1 = relu_err(X_train, X_test, Y_train, Y_test, weights_relu1,dim_layers1)\n",
    "test_err_relu2, train_err_relu2 = relu_err(X_train, X_test, Y_train, Y_test, weights_relu2,dim_layers2)\n",
    "test_err_relu3, train_err_relu3 = relu_err(X_train, X_test, Y_train, Y_test, weights_relu3,dim_layers3)\n",
    "test_err_relu4, train_err_relu4 = relu_err(X_train, X_test, Y_train, Y_test, weights_relu4,dim_layers4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of iteration costs for various hidden layer dimensions\n",
    "\n",
    "iter = np.arange(1,1001)\n",
    "figure = plt.figure(figsize=(7,6))\n",
    "ax = figure.add_subplot(1,1,1)\n",
    "ax.plot(iter,costs_relu1,'r',label='1 hidden layer of 20 unit')\n",
    "ax.plot(iter,costs_relu2,'g',label='2 hidden layers of 10 units')\n",
    "ax.plot(iter,costs_relu3,'b',label='3 hidden layers of 5 units')\n",
    "ax.plot(iter,costs_relu4,'orange',label='3 hidden layers of 10 units')\n",
    "ax.set_xlabel('Iterations');ax.set_ylabel('Costs');ax.set_title('Cost reduction using ReLU function');ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows that the 1 hidden layer configuration with 20 units coverges fastest but starts off with the error higher than the configuration using 2 hidden layers, which allows the 2 hidden layer configuration to reach a lower cost value in 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of errors for various hidden layer dimensions\n",
    "\n",
    "figure = plt.figure(figsize=(7,6))\n",
    "ax = figure.add_subplot(1,1,1)\n",
    "ax.scatter(x=test_err_relu1,y=train_err_relu1,c='r',label='1 hidden layer of 20 unit')\n",
    "ax.scatter(x=test_err_relu2,y=train_err_relu2,c='g',label='2 hidden layers of 10 units')\n",
    "ax.scatter(x=test_err_relu3,y=train_err_relu3,c='b',label='3 hidden layers of 5 units')\n",
    "ax.scatter(x=test_err_relu4,y=train_err_relu4,c='orange',label='3 hidden layers of 10 units')\n",
    "ax.set_xlabel('Train error');ax.set_ylabel('Test error');ax.set_title('Errors using relu function');ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 1000 iterations, the 2 hidden layer configuration using the ReLU activation function has the least error even though it converges slower than using 1 hidden layer. This is due to the 2 hidden layer configuration starting off at a lower cost value than the 1 hidden layer configuration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above implementations, the trend seems to be that using a higher number of units increases accuracy and decrerases convergence rate across the board. All the activation functions produce good results from just using a single hidden layer, this can be explained by the simplicity of the data used and the fact that only a single output feature is being predicted by the model. \n",
    "\n",
    "The various activation functions coverge to different accuracy values after 1000 iterations, the most accurate out of the four used was a close call between ReLU and Linear activations functions. This can be due to the linear relationships present within the data as shown by the Data Wrangling section of the project, indicating that the choice of activation fucntion is depenedent on the problem type."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('base': conda)",
   "name": "python370jvsc74a57bd01f2d5af64179441e9a4bf3dccdbb51cd9560210c5aef7dd373670db26f48b947"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
